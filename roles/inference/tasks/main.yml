---
- name: Update apt cache
  apt:
    update_cache: yes
  become: yes

- name: Check if ROCm is installed
  stat:
    path: /opt/rocm
  register: rocm_installed

- name: Install dependencies
  apt:
    name:
      - build-essential
      - cmake
      - git
      - wget
      - python3
      - python3-venv
      - libopenblas-dev
      - libcurl4-openssl-dev
      - libgtk-3-dev
    state: present
  become: yes

- name: Clone llama.cpp repository
  git:
    repo: https://github.com/ggerganov/llama.cpp.git
    dest: /opt/llama.cpp
    version: HEAD
  become: yes
  register: llama_cloned

- name: Check if llama-cli binary exists
  stat:
    path: /opt/llama.cpp/build/bin/llama-cli
  register: llama_binary_exists

- name: Remove existing build directory
  file:
    path: /opt/llama.cpp/build
    state: absent
  become: yes
  when: llama_cloned.changed or not llama_binary_exists.stat.exists

- name: Configure llama.cpp build with ROCm support
  command: cmake -B build -DGGML_HIP=ON -DGGML_OPENBLAS=OFF -DAMDGPU_TARGETS=gfx1100
  args:
    chdir: /opt/llama.cpp
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    HIPCXX: "/opt/rocm/lib/llvm/bin/clang++"
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes
  when: rocm_installed.stat.exists and (llama_cloned.changed or not llama_binary_exists.stat.exists)

- name: Configure llama.cpp build without ROCm support
  command: cmake -B build -DGGML_OPENBLAS=ON
  args:
    chdir: /opt/llama.cpp
  become: yes
  when: not rocm_installed.stat.exists and (llama_cloned.changed or not llama_binary_exists.stat.exists)

- name: Build llama.cpp with ROCm support
  command: cmake --build build -j{{ ansible_processor_vcpus }}
  args:
    chdir: /opt/llama.cpp
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    HIPCXX: "/opt/rocm/lib/llvm/bin/clang++"
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes
  when: rocm_installed.stat.exists and (llama_cloned.changed or not llama_binary_exists.stat.exists)

- name: Build llama.cpp without ROCm support
  command: cmake --build build -j{{ ansible_processor_vcpus }}
  args:
    chdir: /opt/llama.cpp
  become: yes
  when: not rocm_installed.stat.exists and (llama_cloned.changed or not llama_binary_exists.stat.exists)


- name: Remove distro btop package if present (ROCm systems)
  apt:
    name: btop
    state: absent
  become: yes
  when: rocm_installed.stat.exists

- name: Clone btop repository (ROCm systems)
  git:
    repo: https://github.com/aristocratos/btop.git
    dest: /opt/btop
    version: HEAD
  become: yes
  when: rocm_installed.stat.exists

- name: Build btop with ROCm GPU support
  command: make GPU_SUPPORT=true ROCM_PATH=/opt/rocm
  args:
    chdir: /opt/btop
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes
  when: rocm_installed.stat.exists

- name: Install btop with ROCm support
  command: make install PREFIX=/usr/local
  args:
    chdir: /opt/btop
  become: yes
  when: rocm_installed.stat.exists

- name: Install additional monitoring tools
  apt:
    name:
      - htop
      - iotop
      - powertop
      - sysstat
      - s-tui
      - stress
    state: present
  become: yes

- name: Install btop package (non-ROCm systems)
  apt:
    name: btop
    state: present
  become: yes
  when: not rocm_installed.stat.exists


- name: Add user to render and video groups for GPU access
  user:
    name: system
    groups: render,video
    append: yes
  become: yes
  when: rocm_installed.stat.exists

- name: Create symlinks in /usr/local/bin for easy access
  file:
    src: /opt/llama.cpp/build/bin/{{ item }}
    dest: /usr/local/bin/{{ item }}
    state: link
  become: yes
  loop:
    - llama-cli
    - llama-server