---
# Update system packages
- name: Update apt cache
  apt:
    update_cache: yes
  become: yes
  tags: inference

# Install basic dependencies
- name: Install dependencies
  apt:
    name:
      - build-essential
      - cmake
      - git
      - wget
      - python3
      - python3-venv
      - libopenblas-dev
      - libcurl4-openssl-dev
      - libgtk-3-dev
      - lm-sensors
      - linux-tools-common
      - linux-tools-generic
    state: present
  become: yes

# ROCm Installation - Always from scratch
- name: Remove all existing ROCm-related packages
  shell: apt-get remove --purge -y $(dpkg -l | grep -E "(rocm|hip)" | awk '{print $2}') || true
  become: yes
  failed_when: false

- name: Clean package cache and autoremove
  apt:
    autoremove: yes
    autoclean: yes
    update_cache: yes
  become: yes

- name: Remove existing ROCm directory
  file:
    path: /opt/rocm
    state: absent
  become: yes

- name: Add AMD ROCm repository key
  apt_key:
    url: https://repo.radeon.com/rocm/rocm.gpg.key
    state: present
  become: yes

- name: Add AMD ROCm repository
  apt_repository:
    repo: "deb [arch=amd64] https://repo.radeon.com/rocm/apt/6.4.2 noble main"
    state: present
    update_cache: yes
  become: yes

- name: Create apt preferences for ROCm packages
  copy:
    content: |
      Package: *
      Pin: origin repo.radeon.com
      Pin-Priority: 1001
    dest: /etc/apt/preferences.d/rocm
  become: yes

- name: Install ROCm packages for RX 7900 XT/XTX
  apt:
    name:
      - rocm
      - rocm-hip-sdk
      - rocblas
      - hipblas
      - hipblaslt
      - rocwmma-dev
    state: present
    update_cache: yes
    force: yes
  become: yes

- name: Create ROCm environment configuration
  lineinfile:
    path: /etc/environment
    line: "{{ item }}"
    create: yes
  become: yes
  loop:
    - "ROC_ENABLE_PRE_VEGA=1"
    - "HIP_VISIBLE_DEVICES=0,1"
    - "ROCM_PATH=/opt/rocm"

# llama.cpp Installation - Always from scratch
- name: Remove existing llama.cpp directory
  file:
    path: /opt/llama.cpp
    state: absent
  become: yes

- name: Clone llama.cpp repository from latest HEAD
  git:
    repo: https://github.com/ggerganov/llama.cpp.git
    dest: /opt/llama.cpp
    version: HEAD
    force: yes
  become: yes

- name: Configure llama.cpp build with ROCm support
  command: cmake -B build -DGGML_HIP=ON -DGGML_OPENBLAS=OFF -DAMDGPU_TARGETS=gfx1100
  args:
    chdir: /opt/llama.cpp
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    HIPCXX: "/opt/rocm/lib/llvm/bin/clang++"
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes

- name: Build llama.cpp with ROCm support
  command: cmake --build build -j{{ ansible_processor_vcpus }}
  args:
    chdir: /opt/llama.cpp
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    HIPCXX: "/opt/rocm/lib/llvm/bin/clang++"
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes

# btop Installation - Always from scratch with ROCm support
- name: Remove distro btop package if present
  apt:
    name: btop
    state: absent
  become: yes

- name: Remove existing btop directory
  file:
    path: /opt/btop
    state: absent
  become: yes

- name: Clone btop repository from latest HEAD
  git:
    repo: https://github.com/aristocratos/btop.git
    dest: /opt/btop
    version: HEAD
    force: yes
  become: yes

- name: Build btop with ROCm GPU support
  command: make GPU_SUPPORT=true ROCM_PATH=/opt/rocm
  args:
    chdir: /opt/btop
  environment:
    ROCM_PATH: /opt/rocm
    HIP_PATH: /opt/rocm
    PATH: "/opt/rocm/bin:{{ ansible_env.PATH }}"
  become: yes

- name: Install btop with ROCm support
  command: make install PREFIX=/usr/local
  args:
    chdir: /opt/btop
  become: yes

# Additional monitoring tools
- name: Install additional monitoring tools
  apt:
    name:
      - htop
      - iotop
      - powertop
      - sysstat
      - s-tui
      - stress
    state: present
  become: yes

# User permissions and system configuration
- name: Add user to render and video groups for GPU access
  user:
    name: system
    groups: render,video
    append: yes
  become: yes

# GPU Performance Settings
- name: Set GPU power management to performance mode
  shell: echo "performance" > /sys/class/drm/card{{ item }}/device/power_dpm_force_performance_level
  become: yes
  loop: [0, 1]
  failed_when: false

- name: Set GPU power profile to compute workload
  shell: echo "1" > /sys/class/drm/card{{ item }}/device/pp_compute_power_profile
  become: yes
  loop: [0, 1]
  failed_when: false

- name: Set GPU memory clock to high performance
  shell: echo "high" > /sys/class/drm/card{{ item }}/device/power_dpm_state
  become: yes
  loop: [0, 1]
  failed_when: false

# CPU Performance Settings
- name: Set CPU governor to performance mode
  shell: echo "performance" > /sys/devices/system/cpu/cpu{{ item }}/cpufreq/scaling_governor
  become: yes
  loop: "{{ range(0, ansible_processor_vcpus) | list }}"
  failed_when: false

- name: Set AMD P-State energy performance preference to performance
  shell: echo "performance" > /sys/devices/system/cpu/cpu{{ item }}/cpufreq/energy_performance_preference
  become: yes
  loop: "{{ range(0, ansible_processor_vcpus) | list }}"
  failed_when: false

# Create symlinks for easy access
- name: Create symlinks in /usr/local/bin for easy access
  file:
    src: /opt/llama.cpp/build/bin/{{ item }}
    dest: /usr/local/bin/{{ item }}
    state: link
    force: yes
  become: yes
  loop:
    - llama-cli
    - llama-server

# Verification
- name: Verify ROCm installation
  command: rocminfo
  register: rocm_info
  become: yes
  failed_when: false

- name: Check for gfx1100 GPU target
  shell: rocminfo | grep gfx | head -1
  register: gpu_target
  become: yes
  failed_when: false

- name: Display GPU target information
  debug:
    msg: "GPU Target: {{ gpu_target.stdout }}"
  when: gpu_target.stdout is defined

- name: Display installation completion status
  debug:
    msg: "Inference environment installation completed. All components built from latest sources."