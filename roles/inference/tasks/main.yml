---
- name: Update apt cache
  apt:
    update_cache: yes
  become: yes

- name: Install dependencies
  apt:
    name:
      - build-essential
      - cmake
      - git
      - wget
      - python3
      - python3-venv
      - libopenblas-dev
      - libcurl4-openssl-dev
      - lm-sensors
      - linux-tools-common
      - linux-tools-generic
      - libgtk-3-dev
    state: present
  become: yes

- name: Clone llama.cpp repository
  git:
    repo: https://github.com/ggerganov/llama.cpp.git
    dest: /opt/llama.cpp
    version: HEAD
  become: yes

- name: Configure llama.cpp build with CMake
  command: cmake -B build -DLLAMA_OPENBLAS=ON -DGGML_CPU_AARCH64=OFF
  args:
    chdir: /opt/llama.cpp
  become: yes

- name: Build llama.cpp with CMake
  command: cmake --build build -j{{ ansible_processor_vcpus }}
  args:
    chdir: /opt/llama.cpp
  become: yes

- name: Set CPU governor to performance mode
  shell: echo "performance" > /sys/devices/system/cpu/cpu{{ item }}/cpufreq/scaling_governor
  become: yes
  loop: "{{ range(0, ansible_processor_vcpus) | list }}"
  failed_when: false

- name: Set AMD P-State energy performance preference to performance
  shell: echo "performance" > /sys/devices/system/cpu/cpu{{ item }}/cpufreq/energy_performance_preference
  become: yes
  loop: "{{ range(0, ansible_processor_vcpus) | list }}"
  failed_when: false

- name: Install additional monitoring tools
  apt:
    name:
      - htop
      - iotop
      - powertop
      - sysstat
      - s-tui
      - stress
    state: present
  become: yes


- name: Create symlinks in /usr/local/bin for easy access
  file:
    src: /opt/llama.cpp/build/bin/{{ item }}
    dest: /usr/local/bin/{{ item }}
    state: link
  become: yes
  loop:
    - llama-cli
    - llama-server